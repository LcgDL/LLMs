{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# <h3 align=\"center\"> $\\underline{\\text{ Large Language Model (LLM) }}$</h3>\n",
        "\n",
        "<h3 align=\"center\">$ \\text{Artificial Intelligence (AI) algorithm that uses deep learning techniques.}$</h3>\n",
        "\n"
      ],
      "metadata": {
        "id": "Xg4kloqUP168"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using Self-attention"
      ],
      "metadata": {
        "id": "HwwCR-od40N1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Mathematical technique: matrix multiplication\n",
        "\n",
        "Self-attentio: the keys and values are produced from the same source as queries\n",
        "\n"
      ],
      "metadata": {
        "id": "ELfkyS73NKXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/NLP/neruda_20poemasyCD.txt','r',encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "DQF6qgeLBo1Z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as Tnn\n",
        "from torch.nn import functional as TF"
      ],
      "metadata": {
        "id": "2r6wIXxHDe_t"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B = 4 # batch\n",
        "T = 8 # time\n",
        "C = 32 # channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "head_size = 16 #single Head perform self-attention\n",
        "key = Tnn.Linear(C, head_size, bias=False)\n",
        "query = Tnn.Linear(C, head_size, bias=False)\n",
        "value = Tnn.Linear(C, head_size, bias=False)\n",
        "k,q = key(x), query(x)\n",
        "wei =  q @ k.transpose(-2, -1)\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = TF.softmax(wei, dim=-1)\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "out.shape"
      ],
      "metadata": {
        "id": "46yzuBqiNLdI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0ea70a6-a4b3-46ae-8780-83ef7c64352c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[1]"
      ],
      "metadata": {
        "id": "vbSafFLUNOgt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b20338b0-5897-48a8-d5e6-6f4c3af154cf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
              "        [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
              "        [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "Vd9swRxLJAML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batchS = 16\n",
        "blockS = 32\n",
        "maxIt = 5000\n",
        "evalIv = 100\n",
        "learningRt = 1e-3\n",
        "evalIt = 200\n",
        "embdN = 64\n",
        "headN = 4\n",
        "layerN = 4\n",
        "dropout = 0.0\n",
        "torch.manual_seed(1337)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "_dxKYKi8DOiI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unique characters in this text"
      ],
      "metadata": {
        "id": "W3TjnBiINiO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocabS = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) } #mapping: Ch to Int\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encod = lambda s: [stoi[c] for c in s]          #input:string -> output:integers\n",
        "decod = lambda l: ''.join([itos[i] for i in l]) # input:integers -> output:string\n",
        "data = torch.tensor(encod(text), dtype=torch.long) # Train -> Test\n",
        "n = int(0.9*len(data)) # 90% train\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "W5WknGc3DYVV"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loading"
      ],
      "metadata": {
        "id": "WJ8YD_EZNtcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#batch of data of inputs x and targets y\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - blockS, (batchS,))\n",
        "    x = torch.stack([data[i:i+blockS] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+blockS+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(evalIt)\n",
        "        for k in range(evalIt):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "id-W__5JDssS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Head of self-attention"
      ],
      "metadata": {
        "id": "7eQxsyA4N6pM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HeadSA(Tnn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = Tnn.Linear(embdN, head_size, bias=False)\n",
        "        self.query = Tnn.Linear(embdN, head_size, bias=False)\n",
        "        self.value = Tnn.Linear(embdN, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(blockS, blockS)))\n",
        "\n",
        "        self.dropout = Tnn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = TF.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out"
      ],
      "metadata": {
        "id": "CmwMMUCtD36q"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi self-attention"
      ],
      "metadata": {
        "id": "nS5Xz4uAOLAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHA(Tnn.Module):\n",
        "\n",
        "    def __init__(self, headsN, headS):\n",
        "        super().__init__()\n",
        "        self.heads = Tnn.ModuleList([HeadSA(headS) for _ in range(headsN)])\n",
        "        self.proj = Tnn.Linear(embdN, embdN)\n",
        "        self.dropout = Tnn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "lB47sXRmD33f"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Linear layer followed by a non-linearity"
      ],
      "metadata": {
        "id": "9WIlitnTOSFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(Tnn.Module):\n",
        "\n",
        "    def __init__(self, embdN):\n",
        "        super().__init__()\n",
        "        self.net = Tnn.Sequential(Tnn.Linear(embdN, 4 * embdN),Tnn.ReLU(),\n",
        "            Tnn.Linear(4 * embdN, embdN),Tnn.Dropout(dropout),)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "fUdvGPSyD30E"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer block\n",
        "\n",
        "Communication followed by computation"
      ],
      "metadata": {
        "id": "WXPnIB0AO4r1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(Tnn.Module):\n",
        "\n",
        "    def __init__(self, embdN, headN):\n",
        "        super().__init__()\n",
        "        headS = embdN // headN\n",
        "        self.sa = MultiHA(headN, headS)\n",
        "        self.ffwd = FeedFoward(embdN)\n",
        "        self.ln1 = Tnn.LayerNorm(embdN)\n",
        "        self.ln2 = Tnn.LayerNorm(embdN)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "zc9TDwPJEBuZ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bigram model"
      ],
      "metadata": {
        "id": "_9BiFZdGPiOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramM(Tnn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = Tnn.Embedding(vocabS, embdN)\n",
        "        self.position_embedding_table = Tnn.Embedding(blockS, embdN)\n",
        "        self.blocks = Tnn.Sequential(*[Block(embdN, headN=headN) for _ in range(layerN)])\n",
        "        self.ln_f = Tnn.LayerNorm(embdN)\n",
        "        self.lm_head = Tnn.Linear(embdN, vocabS)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = TF.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, tokensMax):\n",
        "        for _ in range(tokensMax):\n",
        "            idx_cond = idx[:, -blockS:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = TF.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "hyfriXAEEBah"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramM()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learningRt)\n",
        "\n",
        "for iter in range(maxIt):\n",
        "    if iter % evalIv == 0 or iter == maxIt - 1:\n",
        "        losses = estimate_loss()\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decod(m.generate(context, tokensMax=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "eGyQSlfDNOc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89cf1d9b-7ad1-41f5-f8a7-5fb195634352"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.211406 M parameters\n",
            "\n",
            "\n",
            "Pero se van tan los vientos tan los carcoles blantas.\n",
            "\n",
            "ColINADO no idas asio es, las manos noches como una paresta en la boiga como de pesta mi cansa\n",
            "concendo esbanido estabas?\n",
            "Poema 17\n",
            "\n",
            "Puedo escribir los versos más tristes esta noche.\n",
            "\n",
            "Escumos nostán astuy grandioso en irmento.\n",
            "\n",
            "Sejosa, turde, mi camino, desvalida.\n",
            "El hi, detrarque los campos he visesta.\n",
            "\n",
            "Puedo esculo sió-\n",
            "\n",
            "Poema 11\n",
            "\n",
            "\n",
            "Caña mi alma no que se contentas.\n",
            "\n",
            "Ellas están huyendo los pinos sobre una arde les algos,\n",
            "lentiosa de ca abscos del viento.\n",
            "\n",
            "Poema 19\n",
            "\n",
            "Niña morena y ágil, navo que de estatua temerte de tu alma.\n",
            "\n",
            "Poema 12\n",
            "\n",
            "Cayó el libro que van tuyendo mío es son de mistadercerseas en luchas.\n",
            "\n",
            "Ansiedad de pinos sobre tienembla.\n",
            "Ámentada en las humas, orques con la soledad como un puda.\n",
            "\n",
            "Pensandolo, y como un persistión de cada día.\n",
            "\n",
            "A nudse como esta la tarde, igal y mi mi canción de alma.\n",
            "\n",
            "Poy goi voz, altas de todo lo tarderte.\n",
            "Fuién estaba, tu boca como una colaronte.\n",
            "Tu se mi acercarla a mi voz ven tados las cosas.\n",
            "\n",
            "Ansiedade, lejos allá arriba, tristeza que tú me quiso.\n",
            "\fEn las noches como ésta la tuve entre mis brazos.\n",
            "Tú juegas con el amor, y el tiempo maduro,\n",
            "y camino que se adió canta en pasión de callevto.\n",
            "\n",
            "Suerrin como el tiempo de los seon tiños veces bajo ellos\n",
            "y sobre los campos cerrollas, las hojas del correpúsculo\n",
            "\fAlío nado cas:\n",
            "\n",
            "Poy no lamentes mis alas, llamas farenas.\n",
            "\n",
            "Cayón los van flos blancos,\n",
            "y la voz, alegre con hava\n",
            "y hace sol se desenría.\n",
            "Viento de los caympa.\n",
            "Es la hora de las hojas.\n",
            "\n",
            "Ijuer confre de las quevas, mLarobola fuegura de las talgos,\n",
            "y consiSea caía los cortalles de humorantes de repente?\n",
            "Cuando he leejos de fuego\n",
            "el el como un faro.\n",
            "\n",
            "So para que tocarla mi boca ex tus ojos oceánicos.\n",
            "\n",
            "Los pájaros pigofido de mis ojos.\n",
            "\n",
            "Poema 11\n",
            "\n",
            "Cacielo paedra el viento arrastuma\n",
            "\fPasan huyendo los pinos se sonría.\n",
            "Tementado, turnus laminos ríostres de cirues azules.\n",
            "Como el aguastio, mujer del alma de todo,\n",
            "de ti me sea ola ola de mi voz besántas.\n",
            "Mientos eres?\n",
            "\n",
            "Aquí te amo \n"
          ]
        }
      ]
    }
  ]
}