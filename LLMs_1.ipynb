{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# <h3 align=\"center\"> $\\underline{\\text{ Large Language Model (LLM) }}$</h3>\n",
        "\n",
        "<h3 align=\"center\">$ \\text{Artificial Intelligence (AI) algorithm that uses deep learning techniques.}$</h3>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7TN7O85DaMcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a Transformer based language model:\n",
        "Character level language model"
      ],
      "metadata": {
        "id": "vvDi_4xZcT4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: Pablo Neruda -> 20 poemas de amor y una canción desesperada."
      ],
      "metadata": {
        "id": "CV6ex4WoOToa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XXe6Z6KIZgBK"
      },
      "outputs": [],
      "source": [
        "with open('neruda_20poemasyCD.txt','r',encoding='utf-8') as f:\n",
        "  text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"lenth of dataset in characteres: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YS5ZRhPa8BW",
        "outputId": "fb8bc891-46ce-4ee3-cb14-544ef75b5e54"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lenth of dataset in characteres:  21207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7SGMQE4a7-J",
        "outputId": "04298774-8130-45ab-fe9b-66f8d99eb32a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Poema 1\n",
            "\n",
            "Cuerpo de mujer, blancas colinas, muslos blancos,\n",
            "te pareces al mundo en tu actitud de entr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#unique characters enumeration:\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuejtIZMa77D",
        "outputId": "5f26bee6-ee32-43fc-e8c4-4e39f1e656e3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\f !\"(),-.0123456789:;?ABCDEFGHIJLMNOPQRSTUVYZabcdefghijlmnopqrstuvxyzÁÚáéíñóú\n",
            "78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenizers**"
      ],
      "metadata": {
        "id": "qtpvQIR2wzdv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Character level Tokinizer"
      ],
      "metadata": {
        "id": "OEwwTtMs2MGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simple one: mapping from Char to Int and back\n",
        "chToIn = {c:i for i,c in enumerate(chars)}\n",
        "inToCh = {i:c for i,c in enumerate (chars)}\n",
        "encod = lambda x: [chToIn[c] for c in x]          #char_list -> int\n",
        "decod = lambda y: ''.join([inToCh[i]for i in y])  #int -> char_list\n",
        "print(encod(\"Cuerpo de mujer\"))\n",
        "print(decod(encod(\"blancas colinas\")))"
      ],
      "metadata": {
        "id": "mdvzaKWtCP7K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53a8eb3d-628c-4b4d-f1ad-91967a5f178f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[25, 65, 50, 62, 60, 59, 2, 49, 50, 2, 57, 65, 55, 50, 62]\n",
            "blancas colinas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subword level Tokinizer"
      ],
      "metadata": {
        "id": "9Qz-S-wj2PQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTgbszJc6Unc",
        "outputId": "d19fed20-71c0-4250-923f-be00959b2ebb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "tokenizer.tokenize(text[:40])"
      ],
      "metadata": {
        "id": "8142IW-8CP4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd7e436b-fbee-4411-d955-a6735f5b3dee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['poem',\n",
              " '##a',\n",
              " '1',\n",
              " 'cue',\n",
              " '##rp',\n",
              " '##o',\n",
              " 'de',\n",
              " 'mu',\n",
              " '##jer',\n",
              " ',',\n",
              " 'blanca',\n",
              " '##s',\n",
              " 'colin',\n",
              " '##a']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word level Tokinizer"
      ],
      "metadata": {
        "id": "Itld7enL8c7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"tensorflow-text==2.11.*\""
      ],
      "metadata": {
        "id": "euV9Aauq_5mA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OYPxqxya-nY1",
        "outputId": "d391b293-05cb-415d-cdcd-22fa8f5691dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'Poema' b'1' b'Cuerpo' b'de' b'mujer,' b'blancas' b'colinas,' b'muslos'\n",
            " b'blancos,' b'te' b'pareces' b'al' b'mundo' b'en' b'tu' b'actitud' b'de'\n",
            " b'entrega.' b'Mi' b'cuerpo' b'de' b'labriego' b'salvaje' b'te' b'socava'\n",
            " b'y' b'hace' b'saltar' b'el' b'hijo' b'del' b'fondo' b'de' b'la'\n",
            " b'tierra.' b'Fui' b'solo' b'como' b'un' b't\\xc3\\xbanel.'], shape=(40,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_text as tf_text\n",
        "tokenizer = tf_text.WhitespaceTokenizer()\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(tokens[:40])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing the entire trainig set in PyTorch"
      ],
      "metadata": {
        "id": "SodWamvZB_HS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encod(text),dtype=torch.long) #store ina torch.tensor\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIADI3cr8cfn",
        "outputId": "3f94a3d5-c6bd-4d32-aa39-15c87a62783e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([21207]) torch.int64\n",
            "tensor([37, 59, 50, 57, 46,  2, 11,  0,  0, 25, 65, 50, 62, 60, 59,  2, 49, 50,\n",
            "         2, 57, 65, 55, 50, 62,  7,  2, 47, 56, 46, 58, 48, 46, 63,  2, 48, 59,\n",
            "        56, 54, 58, 46, 63,  7,  2, 57, 65, 63, 56, 59, 63,  2, 47, 56, 46, 58,\n",
            "        48, 59, 63,  7,  0, 64, 50,  2, 60, 46, 62, 50, 48, 50, 63,  2, 46, 56,\n",
            "         2, 57, 65, 58, 49, 59,  2, 50, 58,  2, 64, 65,  2, 46, 48, 64, 54, 64,\n",
            "        65, 49,  2, 49, 50,  2, 50, 58, 64, 62, 50, 52, 46,  9,  0, 34, 54,  2,\n",
            "        48, 65, 50, 62, 60, 59,  2, 49, 50,  2, 56, 46, 47, 62, 54, 50, 52, 59,\n",
            "         2, 63, 46, 56, 66, 46, 55, 50,  2, 64, 50,  2, 63, 59, 48, 46, 66, 46,\n",
            "         0, 68,  2, 53, 46, 48, 50,  2, 63, 46, 56, 64, 46, 62,  2, 50, 56,  2,\n",
            "        53, 54, 55, 59,  2, 49, 50, 56,  2, 51, 59, 58, 49, 59,  2, 49, 50,  2,\n",
            "        56, 46,  2, 64, 54, 50, 62, 62, 46,  9,  0,  0, 28, 65, 54,  2, 63, 59,\n",
            "        56, 59,  2, 48, 59, 57, 59,  2, 65, 58,  2, 64, 77, 58, 50, 56,  9,  2,\n",
            "        26, 50,  2, 57, 74,  2, 53, 65, 74, 46, 58,  2, 56, 59, 63,  2, 60, 72,\n",
            "        55, 46, 62, 59, 63,  0, 68,  2, 50, 58,  2, 57, 74,  2, 56, 46,  2, 58,\n",
            "        59, 48, 53, 50,  2, 50, 58, 64, 62, 46, 47, 46,  2, 63, 65,  2, 54, 58,\n",
            "        66, 46, 63, 54, 76, 58,  2, 60, 59, 49, 50, 62, 59, 63, 46,  9,  0, 37,\n",
            "        46, 62, 46,  2, 63, 59, 47, 62, 50, 66, 54, 66, 54, 62, 57, 50,  2, 64,\n",
            "        50,  2, 51, 59, 62, 55, 73,  2, 48, 59, 57, 59,  2, 65, 58,  2, 46, 62,\n",
            "        57, 46,  7,  0, 48, 59, 57, 59,  2, 65, 58, 46,  2, 51, 56, 50, 48, 53,\n",
            "        46,  2, 50, 58,  2, 57, 54,  2, 46, 62, 48, 59,  7,  2, 48, 59, 57, 59,\n",
            "         2, 65, 58, 46,  2, 60, 54, 50, 49, 62, 46,  2, 50, 58,  2, 57, 54,  2,\n",
            "        53, 59, 58, 49, 46,  9,  0,  0, 37, 50, 62, 59,  2, 48, 46, 50,  2, 56,\n",
            "        46,  2, 53, 59, 62, 46,  2, 49, 50,  2, 56, 46,  2, 66, 50, 58, 52, 46,\n",
            "        58, 69, 46,  7,  2, 68,  2, 64, 50,  2, 46, 57, 59,  9,  0, 25, 65, 50,\n",
            "        62, 60, 59,  2, 49, 50,  2, 60, 54, 50, 56,  7,  2, 49, 50,  2, 57, 65,\n",
            "        63, 52, 59,  7,  2, 49, 50,  2, 56, 50, 48, 53, 50,  2, 72, 66, 54, 49,\n",
            "        46,  2, 68,  2, 51, 54, 62, 57, 50,  9,  0, 23, 53,  2, 56, 59, 63,  2,\n",
            "        66, 46, 63, 59, 63,  2, 49, 50, 56,  2, 60, 50, 48, 53, 59,  3,  2, 23,\n",
            "        53,  2, 56, 59, 63,  2, 59, 55, 59, 63,  2, 49, 50,  2, 46, 65, 63, 50,\n",
            "        58, 48, 54, 46,  3,  0, 23, 53,  2, 56, 46, 63,  2, 62, 59, 63, 46, 63,\n",
            "         2, 49, 50, 56,  2, 60, 65, 47, 54, 63,  3,  2, 23, 53,  2, 64, 65,  2,\n",
            "        66, 59, 69,  2, 56, 50, 58, 64, 46,  2, 68,  2, 64, 62, 54, 63, 64, 50,\n",
            "         3,  0,  0, 25, 65, 50, 62, 60, 59,  2, 49, 50,  2, 57, 65, 55, 50, 62,\n",
            "         2, 57, 74, 46,  7,  2, 60, 50, 62, 63, 54, 63, 64, 54, 62, 72,  2, 50,\n",
            "        58,  2, 64, 65,  2, 52, 62, 46, 48, 54, 46,  9,  0, 34, 54,  2, 63, 50,\n",
            "        49,  7,  2, 57, 54,  2, 46, 58, 63, 54, 46,  2, 63, 54, 58,  2, 56, 54,\n",
            "        57, 54, 64, 50,  7,  2, 57, 54,  2, 48, 46, 57, 54, 58, 59,  2, 54, 58,\n",
            "        49, 50, 48, 54, 63, 59,  3,  0, 36, 63, 48, 65, 62, 59, 63,  2, 48, 46,\n",
            "        65, 48, 50, 63,  2, 49, 59, 58, 49, 50,  2, 56, 46,  2, 63, 50, 49,  2,\n",
            "        50, 64, 50, 62, 58, 46,  2, 63, 54, 52, 65, 50,  7,  0, 68,  2, 56, 46,\n",
            "         2, 51, 46, 64, 54, 52, 46,  2, 63, 54, 52, 65, 50,  7,  2, 68,  2, 50,\n",
            "        56,  2, 49, 59, 56, 59, 62,  2, 54, 58, 51, 54, 58, 54, 64, 59,  9,  0,\n",
            "         0, 37, 59, 50, 57, 46,  2, 12,  0,  0, 27, 58,  2, 63, 65,  2, 56, 56,\n",
            "        46, 57, 46,  2, 57, 59, 62, 64, 46, 56,  2, 56, 46,  2, 56, 65, 69,  2,\n",
            "        64, 50,  2, 50, 58, 66, 65, 50, 56, 66, 50,  9,  0, 23, 47, 63, 59, 62,\n",
            "        64, 46,  7,  2, 60, 72, 56, 54, 49, 46,  2, 49, 59, 56, 54, 50, 58, 64,\n",
            "        50,  7,  2, 46, 63, 74,  2, 63, 54, 64, 65, 46, 49, 46,  0, 48, 59, 58,\n",
            "        64, 62, 46,  2, 56, 46, 63,  2, 66, 54, 50, 55, 46, 63,  2, 53, 73, 56,\n",
            "        54, 48, 50, 63,  2, 49, 50, 56,  2, 48, 62, 50, 60, 77, 63, 48, 65, 56,\n",
            "        59,  0, 61, 65, 50,  2, 50, 58,  2, 64, 59, 62, 58, 59,  2, 46,  2, 64,\n",
            "        54,  2, 49, 46,  2, 66, 65, 50, 56, 64, 46, 63,  9,  0,  0, 34, 65, 49,\n",
            "        46,  7,  2, 57, 54,  2, 46, 57, 54, 52, 46,  7,  0, 63, 59, 56, 46,  2,\n",
            "        50, 58,  2, 56, 59,  2, 63, 59, 56, 54, 64, 46, 62, 54, 59,  2, 49, 50,\n",
            "         2, 50, 63, 64, 46,  2, 53, 59, 62, 46,  2, 49, 50,  2, 57, 65, 50, 62,\n",
            "        64, 50, 63,  0, 68,  2, 56, 56, 50, 58, 46,  2, 49, 50,  2, 56, 46, 63,\n",
            "         2, 66, 54, 49, 46, 63,  2, 49, 50, 56])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data into train and validation"
      ],
      "metadata": {
        "id": "OIJxX3zvGp2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data)) #90% = Train, 10 = Val\n",
        "trainD = data[:n]\n",
        "valD = data[n:]"
      ],
      "metadata": {
        "id": "w24eqtgM8ccf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loader"
      ],
      "metadata": {
        "id": "8-gWFk-NT_1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blockSize = 8\n",
        "trainD[:blockSize+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDLwcNoO8cZU",
        "outputId": "f721dfba-22cc-4b61-e635-7af48bcad7a3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([37, 59, 50, 57, 46,  2, 11,  0,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time dimension of the tensors that are going to be feeding into the transformer:"
      ],
      "metadata": {
        "id": "gTYQJ2-NV2cJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = trainD[:blockSize]\n",
        "y = trainD[1:blockSize+1]\n",
        "for t in range(blockSize):\n",
        "  contxt = x[:t+1]\n",
        "  targt = y[t]\n",
        "  print(f\"Input: {contxt} -> Target: {targt}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jSeobVS8cWP",
        "outputId": "c5ed0c74-921d-4f83-e33b-863e911cbeb0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: tensor([37]) -> Target: 59\n",
            "Input: tensor([37, 59]) -> Target: 50\n",
            "Input: tensor([37, 59, 50]) -> Target: 57\n",
            "Input: tensor([37, 59, 50, 57]) -> Target: 46\n",
            "Input: tensor([37, 59, 50, 57, 46]) -> Target: 2\n",
            "Input: tensor([37, 59, 50, 57, 46,  2]) -> Target: 11\n",
            "Input: tensor([37, 59, 50, 57, 46,  2, 11]) -> Target: 0\n",
            "Input: tensor([37, 59, 50, 57, 46,  2, 11,  0]) -> Target: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch dimension: multiple chuncks of text stacked up in a single tensor (4x8)"
      ],
      "metadata": {
        "id": "QrmonQU4Wn6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch = 4\n",
        "block = 8\n",
        "def getBatch(split):\n",
        "  data = trainD if split == 'train' else valD\n",
        "  ix = torch.randint(len(data) - block, (batch,))\n",
        "  x = torch.stack([data[i:i+block]for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block+1]for i in ix])\n",
        "  return x,y\n",
        "\n",
        "xb, yb = getBatch('train')\n",
        "print('input ')\n",
        "print(xb.shape)\n",
        "print(xb,\"\\n\")\n",
        "print('target ')\n",
        "print(yb.shape)\n",
        "print(yb,\"\\n\")\n",
        "for b in range(batch):\n",
        "  for t in range(block):\n",
        "    contxt = xb[b,:t+1]\n",
        "    targt = yb [b,t]\n",
        "    print(f\"Input: {contxt.tolist()} Target -> {targt}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT06dkDL8cTY",
        "outputId": "af60441e-95d9-490d-fee4-4d1da05d8c50"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input \n",
            "torch.Size([4, 8])\n",
            "tensor([[62, 50, 58, 64, 50,  2, 46, 56],\n",
            "        [58,  2, 61, 65, 50,  2, 57, 54],\n",
            "        [65, 55, 50, 62,  7,  2, 47, 56],\n",
            "        [63,  9,  0, 43, 46, 58,  2, 64]]) \n",
            "\n",
            "target \n",
            "torch.Size([4, 8])\n",
            "tensor([[50, 58, 64, 50,  2, 46, 56,  2],\n",
            "        [ 2, 61, 65, 50,  2, 57, 54,  2],\n",
            "        [55, 50, 62,  7,  2, 47, 56, 46],\n",
            "        [ 9,  0, 43, 46, 58,  2, 64, 62]]) \n",
            "\n",
            "Input: [62] Target -> 50\n",
            "Input: [62, 50] Target -> 58\n",
            "Input: [62, 50, 58] Target -> 64\n",
            "Input: [62, 50, 58, 64] Target -> 50\n",
            "Input: [62, 50, 58, 64, 50] Target -> 2\n",
            "Input: [62, 50, 58, 64, 50, 2] Target -> 46\n",
            "Input: [62, 50, 58, 64, 50, 2, 46] Target -> 56\n",
            "Input: [62, 50, 58, 64, 50, 2, 46, 56] Target -> 2\n",
            "Input: [58] Target -> 2\n",
            "Input: [58, 2] Target -> 61\n",
            "Input: [58, 2, 61] Target -> 65\n",
            "Input: [58, 2, 61, 65] Target -> 50\n",
            "Input: [58, 2, 61, 65, 50] Target -> 2\n",
            "Input: [58, 2, 61, 65, 50, 2] Target -> 57\n",
            "Input: [58, 2, 61, 65, 50, 2, 57] Target -> 54\n",
            "Input: [58, 2, 61, 65, 50, 2, 57, 54] Target -> 2\n",
            "Input: [65] Target -> 55\n",
            "Input: [65, 55] Target -> 50\n",
            "Input: [65, 55, 50] Target -> 62\n",
            "Input: [65, 55, 50, 62] Target -> 7\n",
            "Input: [65, 55, 50, 62, 7] Target -> 2\n",
            "Input: [65, 55, 50, 62, 7, 2] Target -> 47\n",
            "Input: [65, 55, 50, 62, 7, 2, 47] Target -> 56\n",
            "Input: [65, 55, 50, 62, 7, 2, 47, 56] Target -> 46\n",
            "Input: [63] Target -> 9\n",
            "Input: [63, 9] Target -> 0\n",
            "Input: [63, 9, 0] Target -> 43\n",
            "Input: [63, 9, 0, 43] Target -> 46\n",
            "Input: [63, 9, 0, 43, 46] Target -> 58\n",
            "Input: [63, 9, 0, 43, 46, 58] Target -> 2\n",
            "Input: [63, 9, 0, 43, 46, 58, 2] Target -> 64\n",
            "Input: [63, 9, 0, 43, 46, 58, 2, 64] Target -> 62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input to the Transformer:"
      ],
      "metadata": {
        "id": "6oUDNft4t3ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTl4SskIta34",
        "outputId": "75a5075a-e6b2-48a2-a4dd-2b75bdd2ffd1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[62, 50, 58, 64, 50,  2, 46, 56],\n",
            "        [58,  2, 61, 65, 50,  2, 57, 54],\n",
            "        [65, 55, 50, 62,  7,  2, 47, 56],\n",
            "        [63,  9,  0, 43, 46, 58,  2, 64]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Neural Network: Bigram Language Model"
      ],
      "metadata": {
        "id": "op1jF2jlt9BB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Simplest Language Model\n",
        "- The tockens are not talking to each other"
      ],
      "metadata": {
        "id": "soUkfDx521VJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "print(decod(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96wg1JaYtawp",
        "outputId": "93dadb38-686d-4596-e9f1-4bdac1c449e5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 78])\n",
            "tensor(4.6538, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "I(;xxÚfvyc?QP(SA\f ílHDCUgBep6S2ñ6N)Gc FPyCcl,Á7hdFn3q!\n",
            "SP3;xñ0ÁerSOt2\n",
            "ái\"(HzESZCx:rNRóH0bEeIi3:C\n",
            "5ÚJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training the Model"
      ],
      "metadata": {
        "id": "6z4FabvS0mW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizer"
      ],
      "metadata": {
        "id": "3faYmAV-0uJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.AdamW(m.parameters(),lr=1e-3)"
      ],
      "metadata": {
        "id": "d__kGZZHtatP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = 32\n",
        "for stp in range(10000):\n",
        "  xb,yb = getBatch('train')\n",
        "  log, loss= m(xb,yb)\n",
        "  opt.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  opt.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9fEYVqH1kBq",
        "outputId": "44dd48ee-fcbc-4f0d-9343-597b88d607a8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2536509037017822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result:"
      ],
      "metadata": {
        "id": "72xiL_vI5p-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(decod(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-RrhZ488cCN",
        "outputId": "9226db30-1bf8-4205-f864-35f880feb2d3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Perestivitunco.\n",
            "\n",
            "Bdos lgr, motamQueres vebare uí.\n",
            "Y pe s, a tuzBU3aú deso seniscocla ndemi os y entú la ltrintues.\n",
            "yasa, bro s\n",
            "\n",
            "Eni lmorbllasechSe enimbl momíAs RFGis a, mosJVigegrtúsu))uejen So. 4\n",
            "y spas, te.\n",
            "Lan erel vasquedola lltoY e, lmua n delge lasquara n to ma qus\n",
            "P8ás.\n",
            "Cula emeabrre s es e)í.\n",
            "\n",
            "\n",
            "Y lambocagra tos e heren ie tea.\n",
            "Asies brere spasinmo strpergun quiegaQEro57\n",
            "toste carons fisostentoschoón ceruete s.\n",
            "\n",
            "UUO tara R3quempe o ve n y pes.\n",
            "y nischoía poso cumeraures\n",
            "homin tr mertes e\n"
          ]
        }
      ]
    }
  ]
}